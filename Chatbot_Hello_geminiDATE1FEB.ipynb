{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXBm3/rXaVJVohKI7Ykz6x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aqeel-DevOps/chat_bot_helo_gemini_1.5_flash/blob/main/Chatbot_Hello_geminiDATE1FEB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NkWjgsiGny0P"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai -q"
      ],
      "metadata": {
        "id": "gJzAMMi9oOTb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "o1Kwdk5xonrK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "ABhdVBnHoz7l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=genai.GenerativeModel(\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "xNdvraCIpNde"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(input(\"Type any thing:\"))\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "9AdaHdp0pjUG",
        "outputId": "6780856a-587a-438f-8f59-36c0919e4a7c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type any thing:WHAT IS PAKISTAN\n",
            "Pakistan is a country in South Asia.  Here are some key aspects:\n",
            "\n",
            "* **Geography:** It borders India to the east, Afghanistan to the west, Iran to the southwest, and China to the northeast.  It also has a coastline along the Arabian Sea.  The geography is diverse, ranging from mountains (the Himalayas and Karakoram) to plains (the Indus River plain) and deserts.\n",
            "\n",
            "* **Government:**  Pakistan is a federal parliamentary republic.  The head of state is the president, and the head of government is the prime minister.\n",
            "\n",
            "* **People:**  Pakistan has a large and diverse population, primarily composed of ethnic Punjabis, Pashtuns, Sindhis, and Muhajirs.  Islam is the dominant religion.\n",
            "\n",
            "* **Culture:**  Pakistan has a rich and varied culture, influenced by its history and geography.  This includes diverse languages, cuisines, music, art, and literature.\n",
            "\n",
            "* **Economy:**  Pakistan's economy is a mix of agriculture, industry, and services.  Major industries include textiles, agriculture (cotton, wheat, rice), and manufacturing.\n",
            "\n",
            "* **History:**  Pakistan was created in 1947 as a separate Muslim state following the partition of British India.  Its history has been marked by periods of both political stability and instability.\n",
            "\n",
            "In short, Pakistan is a large, populous, and geographically diverse country with a complex and significant history and culture.  It is a nation grappling with many challenges, but also possessing significant potential.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"\"\"terms/Parameters to Explain:\n",
        "Messages\n",
        "Model\n",
        "Max Completion Tokens\n",
        "n\n",
        "Stream\n",
        "Temperature\n",
        "Top_p\n",
        "Tools\"\"\")"
      ],
      "metadata": {
        "id": "1VGXR_oZqKka"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRiwD4IBweOr",
        "outputId": "1edec665-8507-426e-a540-eb82715094ae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's break down these terms and parameters commonly used in large language model (LLM) APIs, focusing on their role in controlling the model's output:\n",
            "\n",
            "**1. Messages:**\n",
            "\n",
            "* **Description:** This refers to the input provided to the LLM.  It's typically a structured conversation history, allowing you to provide context and guide the model's response.  The format often involves a list of dictionaries, where each dictionary represents a turn in the conversation, specifying the role (e.g., \"system,\" \"user,\" \"assistant\") and the content of the message.  This is crucial for maintaining context and generating coherent, relevant responses.\n",
            "\n",
            "**2. Model:**\n",
            "\n",
            "* **Description:** This specifies the specific LLM you're using (e.g., `gpt-3.5-turbo`, `text-davinci-003`). Different models have different capabilities, strengths, weaknesses, and costs.  Choosing the right model is essential for achieving your desired results.\n",
            "\n",
            "**3. Max Completion Tokens:**\n",
            "\n",
            "* **Description:**  This parameter limits the length of the model's response. Tokens are the basic units of text processed by the model (roughly corresponding to words or parts of words).  Setting a `max_completion_tokens` value prevents excessively long responses and helps manage costs.\n",
            "\n",
            "**4. n:**\n",
            "\n",
            "* **Description:** This parameter controls the number of independent responses the model generates for a single prompt.  Setting `n=2` would return two different completions, useful for exploring alternative responses or getting a broader range of ideas.\n",
            "\n",
            "**5. Stream:**\n",
            "\n",
            "* **Description:** This parameter determines whether the response is returned as a single, complete string or streamed in real-time. Streaming allows you to receive parts of the response progressively, enabling you to display the text as it's generated, enhancing the user experience, especially for longer responses.\n",
            "\n",
            "**6. Temperature:**\n",
            "\n",
            "* **Description:** This parameter controls the randomness of the model's output.  A `temperature` of 0 will produce the most likely response (deterministic, often repetitive), while a higher temperature (e.g., 1 or more) will make the output more creative, surprising, and potentially less coherent.  It's a crucial parameter for balancing creativity and relevance.\n",
            "\n",
            "**7. Top_p (Nucleus Sampling):**\n",
            "\n",
            "* **Description:**  Similar to temperature, `top_p` controls the randomness of the output, but in a different way.  It considers the most likely tokens whose cumulative probability exceeds `top_p`. This means it selects from a subset of the most probable tokens, balancing diversity and coherence.  Often preferred over temperature because it's less sensitive to extreme values and produces more consistent results.\n",
            "\n",
            "**8. Tools:**\n",
            "\n",
            "* **Description:** This feature is available in more advanced models and allows you to extend the LLM's capabilities by integrating external tools.  These tools could be functions (code that performs specific tasks) or external APIs (allowing access to external data or services). This enables the model to perform actions beyond text generation, such as making web searches, accessing databases, or performing calculations.  It's a powerful feature for creating more sophisticated and functional applications.\n",
            "\n",
            "\n",
            "In summary, these parameters offer fine-grained control over the LLM's behavior, allowing you to tailor its responses to your specific needs and application.  Understanding and appropriately using these parameters is key to effectively leveraging the power of LLMs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8WcKz3Uwi9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}